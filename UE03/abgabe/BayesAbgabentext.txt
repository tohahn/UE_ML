\subsection{Naive Bayes}
Der Gelernte Baum sieht folgendermaßen aus:\\
\texttt{
Root: Classifier \\
Base probabilities: \\
decisionTreeC45: 0.07692308, kNearestNeighbour: 0.07692308, naiveBayes: 0.07692308, decisionTreeID3: 0.07692308, candidateElimination: 0.07692308, \\
Nodes: \\
\\
Trainingsetsize \\
5 x 2 big \\
decisionTreeC45 | low: 0.0 \\
decisionTreeC45 | high: 1.0 \\
kNearestNeighbour | low: 1.0 \\
kNearestNeighbour | high: 0.0 \\
naiveBayes | low: 0.33333334 \\
naiveBayes | high: 0.6666667 \\
decisionTreeID3 | low: 0.6666667 \\
decisionTreeID3 | high: 0.33333334 \\
candidateElimination | low: 0.6666667 \\
candidateElimination | high: 0.33333334 \\
\\
Domain \\
5 x 2 big \\
decisionTreeC45 | discrete: 0.0 \\
decisionTreeC45 | continious: 1.0 \\
kNearestNeighbour | discrete: 0.33333334 \\
kNearestNeighbour | continious: 0.6666667\\
naiveBayes | discrete: 0.0 \\
naiveBayes | continious: 1.0 \\
decisionTreeID3 | discrete: 1.0 \\
decisionTreeID3 | continious: 0.0 \\
candidateElimination | discrete: 1.0 \\
candidateElimination | continious: 0.0 \\
\\
Noise \\
5 x 3 big \\
decisionTreeC45 | none: 1.0 \\
decisionTreeC45 | high: 0.0 \\
decisionTreeC45 | low: 0.0 \\
kNearestNeighbour | none: 0.0 \\
kNearestNeighbour | high: 0.33333334 \\
kNearestNeighbour | low: 0.6666667 \\
naiveBayes | none: 0.0 \\
naiveBayes | high: 1.0 \\
naiveBayes | low: 0.0 \\
decisionTreeID3 | none: 1.0 \\
decisionTreeID3 | high: 0.0 \\
decisionTreeID3 | low: 0.0 \\
candidateElimination | none: 1.0 \\
candidateElimination | high: 0.0 \\
candidateElimination | low: 0.0 \\}
\\
Die Wahrscheinlichkeiten der Zugehörigkeit für die Trainingsdaten zu den jeweiligen Klassifikationsgruppen sieht folgendermaßen aus: \\
\texttt{
(Domain) (Trainingsetsize) (Noise) \\
continious, high, high -> decisionTreeC45 with 0.0 \\
continious, high, high -> kNearestNeighbour with 0.0 \\
continious, high, high -> naiveBayes with 0.15384616 \\
continious, high, high -> decisionTreeID3 with 0.0 \\
continious, high, high -> candidateElimination with 0.0 \\
\\
discrete, high, none -> decisionTreeC45 with 0.0 \\
discrete, high, none -> kNearestNeighbour with 0.0 \\
discrete, high, none -> naiveBayes with 0.0 \\
discrete, high, none -> decisionTreeID3 with 0.07692308 \\
discrete, high, none -> candidateElimination with 0.07692308 \\
\\
continious, high, low -> decisionTreeC45 with 0.0 \\
continious, high, low -> kNearestNeighbour with 0.0 \\
continious, high, low -> naiveBayes with 0.0 \\
continious, high, low -> decisionTreeID3 with 0.0 \\
continious, high, low -> candidateElimination with 0.0 \\}
\\
Auffällig ist dass das letzte Beispiel nicht klassifiziert wird weil die Multiplikation der einzelnen Wahrscheinlichkeiten zusammen immer null ergibt. Dieses Problem des Naive Bayes wurde auch auf den Folien zum Klassifikator auf Seite 42 genannt. Als mögliche Lösung wird vorgeschlagen nur Wahrscheinlichkeiten über Eins zu benutzen. Dies erzeugt zwar mathematisch falsche Wahrscheinlichkeitswerte, erlaubt aber manchmal eine Klassifikation wo konventionell kein Ergebniss erreicht wird.\\
\texttt{
Auf dem letzten Eintrag der Testdaten ergibt sich dann:\\
continious, high, low -> decisionTreeC45 with 4.3076925 \\
continious, high, low -> kNearestNeighbour with 3.4188037 \\
continious, high, low -> naiveBayes with 4.1025643 \\
continious, high, low -> decisionTreeID3 with 1.6410258 \\
continious, high, low -> candidateElimination with 1.6410258 \\
\\
}
Man erkennt dass der Testfall bei dieser Rechnung besser zugeordnet werden kann und sich ein starker Unterschied der Werte zwischen den ersten drei Algorithmen und den letzten zwei erkennen lässt.

